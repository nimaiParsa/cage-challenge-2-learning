{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG import CybORG\n",
    "import inspect\n",
    "from CybORG.Agents import B_lineAgent\n",
    "from CybORG.Agents.Wrappers.ChallengeWrapper import ChallengeWrapper\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS_PER_GAME = 30\n",
    "MAX_EPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup\n"
     ]
    }
   ],
   "source": [
    "print(\"Setup\")\n",
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + f'/Shared/Scenarios/Scenario2.yaml'\n",
    "\n",
    "agent_name = 'Blue'\n",
    "env = ChallengeWrapper(env=CybORG(path, 'sim', agents={'Red': B_lineAgent}), agent_name=agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 52\n",
      "Actions: 145\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "print(f\"States: {states}\")\n",
    "print(f\"Actions: {actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-225.69999999999993\n",
      "Episode:2 Score:-226.2\n",
      "Episode:3 Score:-104.79999999999998\n",
      "Episode:4 Score:-41.5\n",
      "Episode:5 Score:-224.79999999999993\n",
      "Episode:6 Score:-204.79999999999995\n",
      "Episode:7 Score:-226.79999999999993\n",
      "Episode:8 Score:-50.70000000000002\n",
      "Episode:9 Score:-219.69999999999993\n",
      "Episode:10 Score:-47.80000000000002\n",
      "Episode:11 Score:-226.79999999999993\n",
      "Episode:12 Score:-171.79999999999993\n",
      "Episode:13 Score:-202.2\n",
      "Episode:14 Score:-83.79999999999997\n",
      "Episode:15 Score:-27.80000000000001\n"
     ]
    }
   ],
   "source": [
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    for j in range(MAX_STEPS_PER_GAME):\n",
    "        action = random.randint(0, actions-1)\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO model with shared layers and separate actor and critic heads.\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPO, self).__init__()\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, action_dim),\n",
    "            nn.Softmax(dim=-1)  # Outputs probabilities for discrete actions\n",
    "        )\n",
    "\n",
    "        # Critic Network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)  # Outputs state value\n",
    "        )\n",
    "\n",
    "    def act(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.critic(state)\n",
    "        return action_logprobs, state_value.squeeze(), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute returns using Generalized Advantage Estimation (GAE)\n",
    "def compute_returns(rewards, dones, values, next_value, gamma=0.99, lam=0.95):\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    values = values + [next_value]\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "lr = 0.002\n",
    "max_episodes = 10000\n",
    "max_steps = 100  # or as defined by your environment\n",
    "update_timestep = 1000  # Number of timesteps to collect before an update\n",
    "eps_clip = 0.2\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "K_epochs = 6  # PPO update iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50 Avg Reward: -714.9020000000007\n",
      "Episode: 100 Avg Reward: -613.5380000000001\n",
      "Episode: 150 Avg Reward: -661.5920000000004\n",
      "Episode: 200 Avg Reward: -637.3300000000004\n",
      "Episode: 250 Avg Reward: -659.0760000000006\n",
      "Episode: 300 Avg Reward: -573.1620000000003\n",
      "Episode: 350 Avg Reward: -564.3980000000004\n",
      "Episode: 400 Avg Reward: -567.8320000000004\n",
      "Episode: 450 Avg Reward: -522.0720000000006\n",
      "Episode: 500 Avg Reward: -577.2460000000002\n",
      "Episode: 550 Avg Reward: -659.1340000000004\n",
      "Episode: 600 Avg Reward: -661.6440000000007\n",
      "Episode: 650 Avg Reward: -474.3800000000002\n",
      "Episode: 700 Avg Reward: -426.03400000000033\n",
      "Episode: 750 Avg Reward: -472.19400000000024\n",
      "Episode: 800 Avg Reward: -474.3160000000002\n",
      "Episode: 850 Avg Reward: -490.34600000000023\n",
      "Episode: 900 Avg Reward: -632.7260000000002\n",
      "Episode: 950 Avg Reward: -382.8120000000001\n",
      "Episode: 1000 Avg Reward: -497.64200000000005\n",
      "Episode: 1050 Avg Reward: -502.1620000000001\n",
      "Episode: 1100 Avg Reward: -435.1420000000003\n",
      "Episode: 1150 Avg Reward: -446.2320000000002\n",
      "Episode: 1200 Avg Reward: -384.75800000000004\n",
      "Episode: 1250 Avg Reward: -448.3180000000001\n",
      "Episode: 1300 Avg Reward: -409.4620000000001\n",
      "Episode: 1350 Avg Reward: -607.5680000000003\n",
      "Episode: 1400 Avg Reward: -385.21\n",
      "Episode: 1450 Avg Reward: -493.01400000000035\n",
      "Episode: 1500 Avg Reward: -559.3160000000004\n",
      "Episode: 1550 Avg Reward: -479.7280000000003\n",
      "Episode: 1600 Avg Reward: -367.978\n",
      "Episode: 1650 Avg Reward: -485.60600000000034\n",
      "Episode: 1700 Avg Reward: -435.7380000000001\n",
      "Episode: 1750 Avg Reward: -449.9160000000001\n",
      "Episode: 1800 Avg Reward: -490.37800000000027\n",
      "Episode: 1850 Avg Reward: -485.3620000000005\n",
      "Episode: 1900 Avg Reward: -445.9819999999999\n",
      "Episode: 1950 Avg Reward: -485.6340000000003\n",
      "Episode: 2000 Avg Reward: -415.7660000000001\n",
      "Episode: 2050 Avg Reward: -431.7760000000001\n",
      "Episode: 2100 Avg Reward: -455.30400000000026\n",
      "Episode: 2150 Avg Reward: -379.0000000000002\n",
      "Episode: 2200 Avg Reward: -548.2720000000006\n",
      "Episode: 2250 Avg Reward: -444.0960000000004\n",
      "Episode: 2300 Avg Reward: -504.5700000000002\n",
      "Episode: 2350 Avg Reward: -521.8940000000005\n",
      "Episode: 2400 Avg Reward: -540.0520000000002\n",
      "Episode: 2450 Avg Reward: -481.72600000000034\n",
      "Episode: 2500 Avg Reward: -444.9120000000001\n",
      "Episode: 2550 Avg Reward: -375.86199999999997\n",
      "Episode: 2600 Avg Reward: -451.09200000000027\n",
      "Episode: 2650 Avg Reward: -398.3880000000003\n",
      "Episode: 2700 Avg Reward: -367.41799999999995\n",
      "Episode: 2750 Avg Reward: -414.4280000000001\n",
      "Episode: 2800 Avg Reward: -521.5260000000002\n",
      "Episode: 2850 Avg Reward: -370.5540000000001\n",
      "Episode: 2900 Avg Reward: -411.8240000000003\n",
      "Episode: 2950 Avg Reward: -467.79600000000005\n",
      "Episode: 3000 Avg Reward: -538.0060000000003\n",
      "Episode: 3050 Avg Reward: -376.65200000000004\n",
      "Episode: 3100 Avg Reward: -402.7859999999999\n",
      "Episode: 3150 Avg Reward: -428.472\n",
      "Episode: 3200 Avg Reward: -501.5680000000004\n",
      "Episode: 3250 Avg Reward: -416.9180000000002\n",
      "Episode: 3300 Avg Reward: -422.62600000000026\n",
      "Episode: 3350 Avg Reward: -367.24799999999993\n",
      "Episode: 3400 Avg Reward: -492.4700000000004\n",
      "Episode: 3450 Avg Reward: -490.42800000000017\n",
      "Episode: 3500 Avg Reward: -562.8080000000003\n",
      "Episode: 3550 Avg Reward: -508.72200000000043\n",
      "Episode: 3600 Avg Reward: -520.0840000000004\n",
      "Episode: 3650 Avg Reward: -438.1300000000003\n",
      "Episode: 3700 Avg Reward: -327.51799999999986\n",
      "Episode: 3750 Avg Reward: -571.1540000000001\n",
      "Episode: 3800 Avg Reward: -594.1940000000002\n",
      "Episode: 3850 Avg Reward: -656.7300000000006\n",
      "Episode: 3900 Avg Reward: -553.4620000000004\n",
      "Episode: 3950 Avg Reward: -521.4820000000003\n",
      "Episode: 4000 Avg Reward: -421.06600000000014\n",
      "Episode: 4050 Avg Reward: -420.5900000000002\n",
      "Episode: 4100 Avg Reward: -446.40800000000013\n",
      "Episode: 4150 Avg Reward: -482.6700000000003\n",
      "Episode: 4200 Avg Reward: -511.82800000000026\n",
      "Episode: 4250 Avg Reward: -434.19800000000015\n",
      "Episode: 4300 Avg Reward: -443.6020000000002\n",
      "Episode: 4350 Avg Reward: -425.96000000000015\n",
      "Episode: 4400 Avg Reward: -401.33000000000015\n",
      "Episode: 4450 Avg Reward: -439.4900000000001\n",
      "Episode: 4500 Avg Reward: -343.3919999999999\n",
      "Episode: 4550 Avg Reward: -709.2780000000006\n",
      "Episode: 4600 Avg Reward: -654.3060000000003\n",
      "Episode: 4650 Avg Reward: -615.2200000000003\n",
      "Episode: 4700 Avg Reward: -551.3600000000005\n",
      "Episode: 4750 Avg Reward: -538.3340000000003\n",
      "Episode: 4800 Avg Reward: -507.6040000000005\n",
      "Episode: 4850 Avg Reward: -397.1440000000002\n",
      "Episode: 4900 Avg Reward: -456.80200000000025\n",
      "Episode: 4950 Avg Reward: -451.7020000000003\n",
      "Episode: 5000 Avg Reward: -396.10800000000023\n",
      "Episode: 5050 Avg Reward: -433.03200000000004\n",
      "Episode: 5100 Avg Reward: -438.35200000000043\n",
      "Episode: 5150 Avg Reward: -513.7520000000003\n",
      "Episode: 5200 Avg Reward: -432.0680000000001\n",
      "Episode: 5250 Avg Reward: -425.1660000000003\n",
      "Episode: 5300 Avg Reward: -390.9120000000001\n",
      "Episode: 5350 Avg Reward: -385.68399999999997\n",
      "Episode: 5400 Avg Reward: -401.8160000000004\n",
      "Episode: 5450 Avg Reward: -372.7460000000002\n",
      "Episode: 5500 Avg Reward: -403.80600000000027\n",
      "Episode: 5550 Avg Reward: -443.77400000000046\n",
      "Episode: 5600 Avg Reward: -596.2380000000003\n",
      "Episode: 5650 Avg Reward: -442.53200000000027\n",
      "Episode: 5700 Avg Reward: -464.82400000000024\n",
      "Episode: 5750 Avg Reward: -476.7460000000002\n",
      "Episode: 5800 Avg Reward: -412.3500000000002\n",
      "Episode: 5850 Avg Reward: -506.0460000000002\n",
      "Episode: 5900 Avg Reward: -548.2220000000004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Select an action using the current policy\u001b[39;00m\n\u001b[32m     18\u001b[39m action, log_prob = ppo.act(state.unsqueeze(\u001b[32m0\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m next_state, reward, done, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m next_state = torch.FloatTensor(next_state)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Save transition in memory: (state, action, log_prob, reward, done)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Agents/Wrappers/ChallengeWrapper.py:29\u001b[39m, in \u001b[36mChallengeWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m,action=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     obs, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m.step_counter += \u001b[32m1\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step_counter >= \u001b[38;5;28mself\u001b[39m.max_steps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Agents/Wrappers/OpenAIGymWrapper.py:27\u001b[39m, in \u001b[36mOpenAIGymWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> (\u001b[38;5;28mobject\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mself\u001b[39m.action = action\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     result.observation = \u001b[38;5;28mself\u001b[39m.observation_change(result.observation)\n\u001b[32m     29\u001b[39m     result.action_space = \u001b[38;5;28mself\u001b[39m.action_space_change(result.action_space)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Agents/Wrappers/EnumActionWrapper.py:20\u001b[39m, in \u001b[36mEnumActionWrapper.step\u001b[39m\u001b[34m(self, agent, action)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     19\u001b[39m     action = \u001b[38;5;28mself\u001b[39m.possible_actions[action]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Agents/Wrappers/BaseWrapper.py:16\u001b[39m, in \u001b[36mBaseWrapper.step\u001b[39m\u001b[34m(self, agent, action)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent=\u001b[38;5;28;01mNone\u001b[39;00m, action=\u001b[38;5;28;01mNone\u001b[39;00m) -> Results:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     result.observation = \u001b[38;5;28mself\u001b[39m.observation_change(result.observation)\n\u001b[32m     18\u001b[39m     result.action_space = \u001b[38;5;28mself\u001b[39m.action_space_change(result.action_space)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Agents/Wrappers/BlueTableWrapper.py:29\u001b[39m, in \u001b[36mBlueTableWrapper.step\u001b[39m\u001b[34m(self, agent, action)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent=\u001b[38;5;28;01mNone\u001b[39;00m, action=\u001b[38;5;28;01mNone\u001b[39;00m) -> Results:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     obs = result.observation\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m agent == \u001b[33m'\u001b[39m\u001b[33mBlue\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Agents/Wrappers/BaseWrapper.py:16\u001b[39m, in \u001b[36mBaseWrapper.step\u001b[39m\u001b[34m(self, agent, action)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent=\u001b[38;5;28;01mNone\u001b[39;00m, action=\u001b[38;5;28;01mNone\u001b[39;00m) -> Results:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     result.observation = \u001b[38;5;28mself\u001b[39m.observation_change(result.observation)\n\u001b[32m     18\u001b[39m     result.action_space = \u001b[38;5;28mself\u001b[39m.action_space_change(result.action_space)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/CybORG.py:104\u001b[39m, in \u001b[36mCybORG.step\u001b[39m\u001b[34m(self, agent, action, skip_valid_action_check)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m, action=\u001b[38;5;28;01mNone\u001b[39;00m, skip_valid_action_check: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Results:\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Performs a step in CybORG for the given agent.\u001b[39;00m\n\u001b[32m     90\u001b[39m \n\u001b[32m     91\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m \u001b[33;03m        the result of agent performing the action\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvironment_controller\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_valid_action_check\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Shared/EnvironmentController.py:134\u001b[39m, in \u001b[36mEnvironmentController.step\u001b[39m\u001b[34m(self, agent, action, skip_valid_action_check)\u001b[39m\n\u001b[32m    131\u001b[39m     next_observation[agent_name] = \u001b[38;5;28mself\u001b[39m._filter_obs(\u001b[38;5;28mself\u001b[39m.execute_action(\u001b[38;5;28mself\u001b[39m.action[agent_name]), agent_name)\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# get true observation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m true_observation = \u001b[38;5;28mself\u001b[39m._filter_obs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_true_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mINFO_DICT\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTrue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m).data\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Blue update step.\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# New idea: run the MONITOR action for the Blue agent, and update the observation.\u001b[39;00m\n\u001b[32m    138\u001b[39m \n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# pass training information to agents\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_name, agent_object \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_interfaces.items():\n\u001b[32m    141\u001b[39m \n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# determine done signal for agent\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Simulator/SimulationController.py:50\u001b[39m, in \u001b[36mSimulationController.get_true_state\u001b[39m\u001b[34m(self, info)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_true_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, info: \u001b[38;5;28mdict\u001b[39m) -> Observation:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_true_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Simulator/State.py:50\u001b[39m, in \u001b[36mState.get_true_state\u001b[39m\u001b[34m(self, info)\u001b[39m\n\u001b[32m     48\u001b[39m         obs = process.get_state()\n\u001b[32m     49\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obs:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m             \u001b[43mtrue_obs\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhostid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhostname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mInterfaces\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info[hostname]:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m info[hostname][\u001b[33m'\u001b[39m\u001b[33mInterfaces\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mAll\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MyPrograms/Courses/Mini-Project/cage-challenge-2/CybORG/CybORG/Shared/Observation.py:87\u001b[39m, in \u001b[36mObservation.add_process\u001b[39m\u001b[34m(self, hostid, pid, parent_pid, process_name, program_name, service_name, username, path, local_port, remote_port, local_address, remote_address, app_protocol, transport_protocol, status, process_type, process_version, vulnerability, properties, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m     new_process[\u001b[33m\"\u001b[39m\u001b[33mKnown Process\u001b[39m\u001b[33m\"\u001b[39m] = process_name\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m program_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     program_name = \u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProgram Name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m program_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(program_name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Initialize PPO model and optimizer\n",
    "ppo = PPO(states, actions)\n",
    "optimizer = optim.Adam(ppo.parameters(), lr=lr)\n",
    "\n",
    "timestep = 0\n",
    "memory = []  # Store trajectories\n",
    "avg_reward = 0\n",
    "\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "        timestep += 1\n",
    "\n",
    "        # Select an action using the current policy\n",
    "        action, log_prob = ppo.act(state.unsqueeze(0))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        \n",
    "        # Save transition in memory: (state, action, log_prob, reward, done)\n",
    "        memory.append((state, action, log_prob, reward, done))\n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # PPO Update\n",
    "        if timestep % update_timestep == 0:\n",
    "            # Unpack memory\n",
    "            states, actions, log_probs, rewards, dones = zip(*memory)\n",
    "            states = torch.stack(states)\n",
    "            actions = torch.LongTensor(actions)\n",
    "            old_log_probs = torch.stack(log_probs).detach()\n",
    "\n",
    "            # Compute state values\n",
    "            with torch.no_grad():\n",
    "                values = ppo.critic(states).squeeze().tolist()\n",
    "                next_value = 0 if done else ppo.critic(state.unsqueeze(0)).item()\n",
    "            \n",
    "            # Compute advantages and returns\n",
    "            returns = compute_returns(rewards, dones, values, next_value, gamma, lam)\n",
    "            returns = torch.FloatTensor(returns)\n",
    "            advantages = returns - torch.FloatTensor(values)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "            \n",
    "            # PPO update\n",
    "            for _ in range(K_epochs):\n",
    "                # Evaluate new policy\n",
    "                new_log_probs, state_values, dist_entropy = ppo.evaluate(states, actions)\n",
    "                ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "                # PPO loss\n",
    "                surr1 = ratios * advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - eps_clip, 1 + eps_clip) * advantages\n",
    "                loss = -torch.min(surr1, surr2).mean() + 0.5 * F.mse_loss(state_values, returns) - 0.01 * dist_entropy.mean()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Reset memory\n",
    "            memory = []\n",
    "            timestep = 0\n",
    "    \n",
    "    avg_reward += episode_reward\n",
    "    if episode % 50 == 0:\n",
    "        avg_reward = avg_reward / 50\n",
    "        print(\"Episode: {} Avg Reward: {}\".format(episode, avg_reward))\n",
    "        avg_reward = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
