{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "# from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:   # collected from old policy\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        self.logprobs = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.states[:]\n",
    "        del self.actions[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "        del self.logprobs[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, action_dim),\n",
    "            nn.Softmax(dim=-1)\t\t\t# For discrete actions, we use softmax policy\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def act(self, state, memory):       \t\t# state (1,8)\n",
    "        action_probs = self.actor(state)        # (1,4)\n",
    "        dist = Categorical(action_probs)\t\t# distribution func: sample an action (return the corresponding index) according to the probs \n",
    "        action = dist.sample()          \t\t\n",
    "        action_logprob = dist.log_prob(action)  # (1,)\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob)\n",
    "        # print(action_probs.size(), action_logprob.size(), action.size())\n",
    "        return action.item()\t# convert to scalar\n",
    "\n",
    "    def evaluate(self, state, action):      # state (2000, 8); action (2000, 4)\n",
    "        state_value = self.critic(state)    # (2000, 1)\n",
    "\n",
    "        # to calculate action score(logprobs) and distribution entropy\n",
    "        action_probs = self.actor(state)    # (2000,4)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\t# (2000, 1)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr, betas, gamma, K_epochs, eps_clip, restore=False, ckpt=None):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        # current policy\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        if restore:\n",
    "            pretained_model = torch.load(ckpt, map_location=lambda storage, loc: storage)\n",
    "            self.policy.load_state_dict(pretained_model)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "        # old policy: initialize old policy with current policy's parameter\n",
    "        self.old_policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MSE_loss = nn.MSELoss()\t# to calculate critic loss\n",
    "\n",
    "    def select_action(self, state, memory):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)  # flatten the state\n",
    "        return self.old_policy.act(state, memory)\n",
    "\n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimation of rewards\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + self.gamma * discounted_reward\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalize rewards\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(memory.states).to(device)).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(memory.actions).to(device)).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()\n",
    "\n",
    "        # Train policy for K epochs: sampling and updating\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluate old actions and values using current policy\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Importance ratio: p/q\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Advantages\n",
    "            advantages = rewards - state_values.detach()  # old states' rewards - old states' value( evaluated by current policy)\n",
    "\n",
    "            # Actor loss using Surrogate loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            actor_loss = - torch.min(surr1, surr2)\n",
    "\n",
    "            # Critic loss: critic loss - entropy\n",
    "            critic_loss = 0.5 * self.MSE_loss(rewards, state_values) - 0.01 * dist_entropy\n",
    "\n",
    "            # Total loss\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            # Backward gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights to old_policy\n",
    "        self.old_policy.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_name, env, state_dim, action_dim, render, solved_reward, max_episodes, max_timesteps, update_timestep, K_epochs, eps_clip, gamma, lr, betas, ckpt_folder, restore, tb=False, print_interval=10, save_interval=100):\n",
    "    ckpt = ckpt_folder+env_name+'.pth'\n",
    "    if restore:\n",
    "        print('Load checkpoint from {}'.format(ckpt))\n",
    "\n",
    "    memory = Memory()\n",
    "\n",
    "    ppo = PPO(state_dim, action_dim, lr, betas, gamma, K_epochs, eps_clip, restore=restore, ckpt=ckpt)\n",
    "\n",
    "    running_reward, avg_length, time_step = 0, 0, 0\n",
    "\n",
    "    # training loop\n",
    "    for i_episode in range(1, max_episodes+1):\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(max_timesteps):\n",
    "            time_step += 1\n",
    "\n",
    "            # Run old policy\n",
    "            action = ppo.select_action(state, memory)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(done)\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                ppo.update(memory)\n",
    "                memory.clear_memory()\n",
    "                time_step = 0\n",
    "\n",
    "            running_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        avg_length += t\n",
    "\n",
    "        # if running_reward > (print_interval * solved_reward):\n",
    "        #     print(\"########## Solved! ##########\")\n",
    "        #     torch.save(ppo.policy.state_dict(), ckpt_folder + '/PPO_discrete_{}.pth'.format(env_name))\n",
    "        #     print('Save a checkpoint!')\n",
    "        #     break\n",
    "\n",
    "        if i_episode % save_interval == 0:\n",
    "            torch.save(ppo.policy.state_dict(), ckpt_folder + '/PPO_discrete_{}.pth'.format(env_name))\n",
    "            print('Save a checkpoint!')\n",
    "\n",
    "        if i_episode % print_interval == 0:\n",
    "            avg_length = int(avg_length / print_interval)\n",
    "            running_reward = int((running_reward / print_interval))\n",
    "\n",
    "            print('Episode {} \\t Avg length: {} \\t Avg reward: {}'.format(i_episode, avg_length, running_reward))\n",
    "\n",
    "            # if tb:\n",
    "            #     writer.add_scalar('scalar/reward', running_reward, i_episode)\n",
    "            #     writer.add_scalar('scalar/length', avg_length, i_episode)\n",
    "\n",
    "            running_reward, avg_length = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CybORG import CybORG\n",
    "import inspect\n",
    "from CybORG.Agents import B_lineAgent\n",
    "from CybORG.Agents.Wrappers.ChallengeWrapper import ChallengeWrapper\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup\n"
     ]
    }
   ],
   "source": [
    "print(\"Setup\")\n",
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + f'/Shared/Scenarios/Scenario2.yaml'\n",
    "\n",
    "agent_name = 'Blue'\n",
    "env = ChallengeWrapper(env=CybORG(path, 'sim', agents={'Red': B_lineAgent}), agent_name=agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 52\n",
      "Actions: 145\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "print(f\"States: {states}\")\n",
    "print(f\"Actions: {actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 \t Avg length: 99 \t Avg reward: -735\n",
      "Episode 100 \t Avg length: 99 \t Avg reward: -707\n",
      "Episode 150 \t Avg length: 99 \t Avg reward: -662\n",
      "Episode 200 \t Avg length: 99 \t Avg reward: -676\n",
      "Episode 250 \t Avg length: 99 \t Avg reward: -668\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    \"PPO\",\n",
    "    env,\n",
    "    states,\n",
    "    actions,\n",
    "    render=False,\n",
    "    solved_reward=-10,\n",
    "    max_episodes=10000,\n",
    "    max_timesteps=100,\n",
    "    update_timestep=2000,\n",
    "    K_epochs=4,\n",
    "    eps_clip=0.2,\n",
    "    gamma=0.99,\n",
    "    lr=0.0003,\n",
    "    betas=(0.9, 0.999),\n",
    "    ckpt_folder='checkpoints',\n",
    "    restore=False,\n",
    "    tb=False,\n",
    "    print_interval=50,\n",
    "    save_interval=1000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
